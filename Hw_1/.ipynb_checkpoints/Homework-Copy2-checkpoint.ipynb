{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "everyday-evolution",
   "metadata": {},
   "source": [
    "# Guitar effect classification \n",
    "\n",
    "\n",
    "Implementation of a SVM classifier system able to predict the presence of a Tremolo or a the distorsion effect applied to a  guitar mono audio recording. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "technological-intranet",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "from librosa import display\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.svm\n",
    "import IPython.display as ipd\n",
    "import scipy as sp\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-protection",
   "metadata": {},
   "source": [
    "Due to the fact that both these effect imply a significat change in the power spectrum, It efficient to use as a decision feature the Root Mean Square the prediction works efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "interstate-residence",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 87.62it/s]\n"
     ]
    }
   ],
   "source": [
    "classes = ['Distortion', 'Tremolo', 'NoFX']\n",
    "n_centroids = 173\n",
    "dict_train_features = {'Distortion': [], 'Tremolo': [], 'NoFX': []}\n",
    "\n",
    "for c in tqdm(classes):\n",
    "    train_root = 'inputs/{}/training/'.format(c)\n",
    "    class_train_files = [f for f in os.listdir(train_root) if f.endswith('.wav')]\n",
    "    n_train_samples = len(class_train_files)\n",
    "    \n",
    "    train_features = np.zeros((n_train_samples, n_centroids))\n",
    "    for index, f in enumerate(class_train_files):\n",
    "        audio, fs = librosa.load(os.path.join(train_root, f), sr=None)\n",
    "        audio = audio/np.max(np.absolute(audio))\n",
    "        centroids = librosa.feature.rms(audio, frame_length=1024, hop_length=512)\n",
    "        train_features[index, :] = centroids\n",
    "        \n",
    "\n",
    "    dict_train_features[c] = train_features    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "married-proceeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for cents in dict_train_features[\"Distortion\"]:\n",
    "    #fig = plt.figure(figsize=(16, 6))\n",
    "    #plt.plot(np.arange(n_centroids), np.transpose(cents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bronze-blackberry",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for cents in dict_train_features[\"Tremolo\"]:\n",
    "    #fig = plt.figure(figsize=(16, 6))\n",
    "    #plt.plot(np.arange(n_centroids), np.transpose(cents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "independent-venture",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 99.62it/s]\n"
     ]
    }
   ],
   "source": [
    "dict_test_features = {'Distortion': [], 'Tremolo': [], 'NoFX': []}\n",
    "\n",
    "for c in tqdm(classes):\n",
    "    test_root = 'inputs/{}/test/'.format(c)\n",
    "    class_test_files = [f for f in os.listdir(test_root) if f.endswith('.wav')]\n",
    "    n_test_samples = len(class_test_files)\n",
    "    \n",
    "    test_features = np.zeros((n_test_samples, n_centroids))\n",
    "    for index, f in enumerate(class_test_files):\n",
    "        audio, fs = librosa.load(os.path.join(test_root, f), sr=None)\n",
    "        audio = audio/np.max(np.absolute(audio))\n",
    "        centroids = librosa.feature.rms(audio, frame_length=1024, hop_length=512)\n",
    "        test_features[index, :] = centroids\n",
    "        \n",
    "    dict_test_features[c] = test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "social-appraisal",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_0 = 'Distortion'\n",
    "class_1 = 'Tremolo'\n",
    "\n",
    "X_train_0 = dict_train_features[class_0]\n",
    "X_train_1 = dict_train_features[class_1]\n",
    "\n",
    "X_train = np.concatenate((X_train_0, X_train_1), axis=0)\n",
    "\n",
    "y_train_0 = np.zeros((X_train_0.shape[0],))\n",
    "y_train_1 = np.ones((X_train_1.shape[0],))\n",
    "\n",
    "y_train = np.concatenate((y_train_0, y_train_1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "chief-semiconductor",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_0 = dict_test_features[class_0]\n",
    "X_test_1 = dict_test_features[class_1]\n",
    "\n",
    "X_test = np.concatenate((X_test_0, X_test_1), axis=0)\n",
    "\n",
    "y_test_0 = np.zeros((X_test_0.shape[0],))\n",
    "y_test_1 = np.ones((X_test_1.shape[0],))\n",
    "\n",
    "y_test = np.concatenate((y_test_0, y_test_1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-microwave",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cardiac-military",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_max = np.max(X_train, axis=0)\n",
    "feat_min = np.min(X_train, axis=0)\n",
    "X_train_normalized = (X_train - feat_min) / (feat_max - feat_min)\n",
    "X_test_normalized = (X_test - feat_min) / (feat_max - feat_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-sewing",
   "metadata": {},
   "source": [
    "# Weighted Support Vector Machine\n",
    "\n",
    "The basic idea is to assign different weights to different data points such that the WSVM training algorithm learns the decision surface according to the relative importance of data points in the training data set.\n",
    "\n",
    "Specifically, each example in the training dataset has its own penalty term (C value) used in the calculation for the margin when fitting the SVM model. The value of an example’s C-value can be calculated as a weighting of the global C-value, where the weight is defined proportional to the class distribution.\n",
    "\n",
    "C_i = weight_i * C\n",
    "\n",
    "By default, each class has the same weighting, which means that the softness of the margin is symmetrical. C stands for the regularization parameter that controls the trade-off between maximizing the separation margin between classes and minimizing the number of misclassified instances. C determines the number and severity of the violations to the margin (and to the hyperplane) that we will tolerate\n",
    "\n",
    "A larger weighting can be used for the minority class, allowing the margin to be softer, whereas a smaller weighting can be used for the majority class, forcing the margin to be harder and preventing misclassified examples.\n",
    "\n",
    "Small Weight: Smaller C value, larger penalty for misclassified examples.\n",
    "\n",
    "Larger Weight: Larger C value, smaller penalty for misclassified examples.\n",
    "\n",
    "This has the effect of encouraging the margin to contain the majority class with less flexibility, but allow the minority class to be flexible with misclassification of majority class examples onto the minority class side if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "brave-client",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_parameters={\n",
    "    'C': 0.5, \n",
    "    'kernel': 'rbf',\n",
    "    'class_weight' : 'balanced' \n",
    "}\n",
    "\n",
    "   \n",
    "clf = sklearn.svm.SVC(**SVM_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "electoral-poetry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=0.5, class_weight='balanced')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train_normalized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "victorian-demonstration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(gt_labels, predicted_labels):\n",
    "    TP = np.sum(np.logical_and(predicted_labels == 1, gt_labels == 1))\n",
    "    FP = np.sum(np.logical_and(predicted_labels == 1, gt_labels == 0))\n",
    "    TN = np.sum(np.logical_and(predicted_labels == 0, gt_labels == 0))\n",
    "    FN = np.sum(np.logical_and(predicted_labels == 0, gt_labels == 1))\n",
    "    accuracy = (TP + TN) / (TP + FP + TN + FN)\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    F1_score = 2 * precision * recall / (precision + recall)\n",
    "    print(\"Results : \\n accuracy = {} \\n precision = {} \\n recall = {} \\n F1 score = {}\".format(\n",
    "        accuracy, precision, recall, F1_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "rotary-nelson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results : \n",
      " accuracy = 1.0 \n",
      " precision = 1.0 \n",
      " recall = 1.0 \n",
      " F1 score = 1.0\n"
     ]
    }
   ],
   "source": [
    "y_test_predicted = clf.predict(X_test_normalized)\n",
    "\n",
    "compute_metrics(y_test, y_test_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "allied-kruger",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_0 = 'Distortion'\n",
    "class_1 = 'Tremolo'\n",
    "class_2 = 'NoFX'\n",
    "\n",
    "X_train_0 = dict_train_features[class_0]\n",
    "X_train_1 = dict_train_features[class_1]\n",
    "X_train_2 = dict_train_features[class_2]\n",
    "\n",
    "y_train_0 = np.zeros((X_train_0.shape[0],))\n",
    "y_train_1 = np.ones((X_train_1.shape[0],))\n",
    "y_train_2 = np.ones((X_train_2.shape[0],))*2\n",
    "\n",
    "#y_train = np.concatenate((y_train_class_0, y_train_class_1, y_train_class_1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "precious-decrease",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_0 = dict_test_features[class_0]\n",
    "X_test_1 = dict_test_features[class_1]\n",
    "X_test_2 = dict_test_features[class_2]\n",
    "\n",
    "\n",
    "y_test_0 = np.zeros((X_test_0.shape[0],))\n",
    "y_test_1 = np.ones((X_test_1.shape[0],))\n",
    "y_test_2 = np.ones((X_test_2.shape[0],))*2\n",
    "\n",
    "y_test_mc = np.concatenate((y_test_0, y_test_1, y_test_2), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "colored-daisy",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_max = np.max(np.concatenate((X_train_0, X_train_1, X_train_2), axis=0), axis=0)\n",
    "feat_min = np.min(np.concatenate((X_train_0, X_train_1, X_train_2), axis=0), axis=0)\n",
    "\n",
    "X_train_0_normalized = (X_train_0 - feat_min) / (feat_max - feat_min)\n",
    "X_train_1_normalized = (X_train_1 - feat_min) / (feat_max - feat_min)\n",
    "X_train_2_normalized = (X_train_2 - feat_min) / (feat_max - feat_min)\n",
    "\n",
    "X_test_0_normalized = (X_test_0 - feat_min) / (feat_max - feat_min)\n",
    "X_test_1_normalized = (X_test_1 - feat_min) / (feat_max - feat_min)\n",
    "X_test_2_normalized = (X_test_2 - feat_min) / (feat_max - feat_min)\n",
    "\n",
    "X_test_mc_normalized = np.concatenate((X_test_0_normalized, X_test_1_normalized, X_test_2_normalized), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "patent-april",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_parameters={\n",
    "    'C': 0.5,\n",
    "    'kernel': 'rbf',\n",
    "    'class_weight' : 'balanced'\n",
    "}\n",
    "\n",
    "clf_01 = sklearn.svm.SVC(**SVM_parameters, probability=True)\n",
    "clf_02 = sklearn.svm.SVC(**SVM_parameters, probability=True)\n",
    "clf_12 = sklearn.svm.SVC(**SVM_parameters, probability=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "vital-bearing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=0.5, class_weight='balanced', probability=True)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_01.fit(np.concatenate((X_train_0_normalized, X_train_1_normalized), axis=0), \n",
    "           np.concatenate((y_train_0, y_train_1), axis=0))\n",
    "           \n",
    "clf_02.fit(np.concatenate((X_train_0_normalized, X_train_2_normalized), axis=0), \n",
    "           np.concatenate((y_train_0, y_train_2), axis=0))\n",
    "\n",
    "clf_12.fit(np.concatenate((X_train_1_normalized, X_train_2_normalized), axis=0), \n",
    "           np.concatenate((y_train_1, y_train_2), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cosmetic-carnival",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predicted_01 = clf_01.predict(X_test_mc_normalized).reshape(-1, 1)\n",
    "y_test_predicted_02 = clf_02.predict(X_test_mc_normalized).reshape(-1, 1)\n",
    "y_test_predicted_12 = clf_12.predict(X_test_mc_normalized).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "remarkable-cutting",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predicted_mc = np.concatenate((y_test_predicted_01, y_test_predicted_02, y_test_predicted_12), axis=1)\n",
    "y_test_predicted_mc = np.array(y_test_predicted_mc, dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "individual-healing",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predicted_mv = np.zeros((y_test_predicted_mc.shape[0],))\n",
    "for i, e in enumerate(y_test_predicted_mc):\n",
    "    y_test_predicted_mv[i] = np.bincount(e).argmax() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "existing-express",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cm_multiclass(gt, predicted):\n",
    "    classes = np.unique(gt)\n",
    "    \n",
    "    CM = np.zeros((len(classes), len(classes)))\n",
    "    \n",
    "    for i in np.arange(len(classes)):\n",
    "        pred_class = predicted[gt==i]\n",
    "        \n",
    "        for j in np.arange(len(pred_class)):\n",
    "            CM[i, int(pred_class[j])] = CM[i, int(pred_class[j])] + 1 \n",
    "    print(CM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "arbitrary-holocaust",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10.  0.  0.]\n",
      " [ 0.  5.  5.]\n",
      " [ 0.  3.  7.]]\n"
     ]
    }
   ],
   "source": [
    "compute_cm_multiclass(y_test_mc, y_test_predicted_mv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "through-treasurer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surrounded-montana",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
